[user]
user-id = yhbian
topo-id = Multi_DataCenter_Experiment
backend-ip = 192.168.1.46
backend-port = 25890

[server-control]
model = llama-3-8B
dataset = WuDao
dtype = fp32
nodes-num = 4
initial-distribution-phase = True
initial-distribution-algorithm = tree_sync_mode
gradient-convergence-algorithm = ring_sync_mode
circle-topo = ["h1", "h2", "h3", "h4"]
nodes-list = ["h1", "h2", "h3", "h4"]
father-node = h1

[wide-area-topo]
port-list = ["9000", "9001", "9002", "9003"]


[node1]
topologies = Ring
npus-count = 4
bandwidth = 50.0
latency = 500.0
scheduling-policy = LIFO
endpoint-delay = 10
active-chunks-per-dimension = 1
preferred-dataset-splits = 4
boost-mode = 0
all-reduce-implementation = halvingDoubling
all-gather-implementation = halvingDoubling
reduce-scatter-implementation = halvingDoubling
all-to-all-implementation = direct
collective-optimization = localBWAware
workload=DLRM_HybridParallel

[node2]
topologies = Ring
npus-count = 4
bandwidth = 50
latency = 500
scheduling-policy = LIFO
endpoint-delay = 10
active-chunks-per-dimension = 1
preferred-dataset-splits = 4
boost-mode = 0
all-reduce-implementation = halvingDoubling
all-gather-implementation = halvingDoubling
reduce-scatter-implementation = halvingDoubling
all-to-all-implementation = direct
collective-optimization = localBWAware
workload=DLRM_HybridParallel


[node3]
topologies = Ring
npus-count = 4
bandwidth = 50
latency = 500
scheduling-policy = LIFO
endpoint-delay = 10
active-chunks-per-dimension = 1
preferred-dataset-splits = 4
boost-mode = 0
all-reduce-implementation = halvingDoubling
all-gather-implementation = halvingDoubling
reduce-scatter-implementation = halvingDoubling
all-to-all-implementation = direct
collective-optimization = localBWAware
workload=DLRM_HybridParallel


[node4]
topologies = Ring
npus-count = 4
bandwidth = 50
latency = 500
scheduling-policy = LIFO
endpoint-delay = 10
active-chunks-per-dimension = 1
preferred-dataset-splits = 4
boost-mode = 0
all-reduce-implementation = halvingDoubling
all-gather-implementation = halvingDoubling
reduce-scatter-implementation = halvingDoubling
all-to-all-implementation = direct
collective-optimization = localBWAware
workload=DLRM_HybridParallel
